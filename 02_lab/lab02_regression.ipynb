{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "lab2_regression_seminar.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2020/blob/master/02_lab/lab02_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCKrupOesLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1LUfwtesLs",
        "colab_type": "text"
      },
      "source": [
        "# Today's data\n",
        "\n",
        "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "melNrhQoesLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "data = fetch_olivetti_faces().images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA2Q-6P2esLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @this code showcases matplotlib subplots. The syntax is: plt.subplot(height, width, index_starting_from_1)\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(data[0],cmap='gray')\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(data[1],cmap='gray')\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(data[2],cmap='gray')\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(data[3],cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbCClhYesL5",
        "colab_type": "text"
      },
      "source": [
        "# Face reconstruction problem\n",
        "\n",
        "Let's solve the face reconstruction problem: given left halves of facex __(X)__, our algorithm shall predict the right half __(y)__. Our first step is to slice the photos into X and y using slices.\n",
        "\n",
        "__Slices in numpy:__\n",
        "* In regular python, slice looks roughly like this: `a[2:5]` _(select elements from 2 to 5)_\n",
        "* Numpy allows you to slice N-dimensional arrays along each dimension: [image_index, height, width]\n",
        "  * `data[:10]` - Select first 10 images\n",
        "  * `data[:, :10]` - For all images, select a horizontal stripe 10 pixels high at the top of the image\n",
        "  * `data[10:20, :, -25:-15]` - Take images [10, 11, ..., 19], for each image select a _vetrical stripe_ of width 10 pixels, 15 pixels away from the _right_ side.\n",
        "\n",
        "__Your task:__\n",
        "\n",
        "Let's use slices to select all __left image halves as X__ and all __right halves as y__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV3HsLhQesL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUUu5T-uesL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select left half of each face as X, right half as Y\n",
        "X = <Slice left half-images>\n",
        "y = <Slice right half-images>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW4F11b_esMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you did everything right, you're gonna see left half-image and right half-image drawn separately in natural order\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(X[0],cmap='gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y[0],cmap='gray')\n",
        "\n",
        "assert X.shape == y.shape == (len(data), 64, 32), \"Please slice exactly the left half-face to X and right half-face to Y\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Rxit8SesMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def glue(left_half,right_half):\n",
        "    # merge photos back together\n",
        "    left_half = left_half.reshape([-1, 64, 32])\n",
        "    right_half = right_half.reshape([-1, 64, 32])\n",
        "    return np.concatenate([left_half, right_half],axis=-1)\n",
        "\n",
        "\n",
        "# if you did everything right, you're gonna see a valid face\n",
        "plt.imshow(glue(X, y)[99], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvCMAogesML",
        "colab_type": "text"
      },
      "source": [
        "# Machine learning stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_ajG2zoesMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.reshape([len(X), -1]),\n",
        "                                                    y.reshape([len(y), -1]),\n",
        "                                                    test_size=0.05, random_state=42)\n",
        "\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3timNAgJesMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPXoDAiqesMW",
        "colab_type": "text"
      },
      "source": [
        "measure mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH2HuGi4esMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Train MSE:\", mean_squared_error(Y_train, model.predict(X_train)))\n",
        "print(\"Test MSE:\", mean_squared_error(Y_test, model.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLvFyMtSfyQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIgCEFkofyMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvibaLEfyIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-e00MeoesMb",
        "colab_type": "text"
      },
      "source": [
        "## Why train error is much smaller than test?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0_7ZZFgAya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and Y for the train dataset\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCiLbOTugSV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and Y for the test dataset\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_k5rCuHgsVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpCbMZ1pgsR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZZF6Ns0gsIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPtPdBCMesMm",
        "colab_type": "text"
      },
      "source": [
        "Remember regularisation? That is exactly what we need. There are many many linear models in sklearn package, and all of them can be found [here](https://scikit-learn.org/stable/modules/linear_model.html). We will focus on 3 of them: Ridge regression, Lasso and ElasticNet.\n",
        "Idea of all of them is very simple: Add some penalty to the objective loss function to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucmL5LNSesMm",
        "colab_type": "text"
      },
      "source": [
        "# Ridge regression\n",
        "RidgeRegression is just a LinearRegression, with l2 regularization - penalized for $ \\alpha \\cdot \\sum _i w_i^2$\n",
        "\n",
        "Let's train such a model with alpha=0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxtafdhesMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from <YOUR CODE> import <YOUR CODE>\n",
        "\n",
        "ridge = <YOUR CODE>\n",
        "\n",
        "<YOUR CODE: fit the model on training set>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0UAnpxQesMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<YOUR CODE: predict and measure MSE on train and test>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgOK0gfFiirK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and Y for the test dataset\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0xpbcZVesMy",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Grid search\n",
        "\n",
        "Train model with diferent $\\alpha$ and find one that has minimal test MSE. It's okay to use loops or any other python stuff here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgy_LgwNesMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ETgdXIesM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_plot(models, parameter_dict):\n",
        "    \"\"\"This function takes a list of models and parameters\n",
        "    dict as input and plot a graph of MSE loss VS parameter value\"\"\"\n",
        "    for model in models: \n",
        "        # use GridSearchCV as before to do grid search\n",
        "        gscv = \n",
        "        <YOUR CODE>\n",
        "        plt.errorbar(gscv.param_grid['alpha'],\n",
        "                     gscv.cv_results_['mean_test_score'],\n",
        "                     gscv.cv_results_['std_test_score'],\n",
        "                     capsize=5, label=model.__str__().split(\"(\")[0])\n",
        "        plt.xscale(\"log\", nonposx='clip')\n",
        "        plt.xlabel(\"alpha\")\n",
        "        plt.ylabel(\"negative MSE\")\n",
        "        plt.grid()\n",
        "        plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjQb9Ky2esM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "models = <YOUR CODE> # Start from Ridge regression, but feel free to add \n",
        "                     # Lasso and ElasticNet. Note that the latter two cannot\n",
        "                     # be solved analytically and typically are much slower\n",
        "                     # to fit than Ridge regression (so you may want to limit\n",
        "                     # the number of grid points).\n",
        "parameter_dict = <YOUR CODE>\n",
        "\n",
        "train_and_plot(models, parameter_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4_oZE-CesM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQGIIuhbesNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test predictions\n",
        "pics = glue(X_test, <predict with your best model>)\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433Mv13AesNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso, ElasticNet\n",
        "\n",
        "# Use the code you have just done to do GridSearch for Lasso and/or ElasticNet\n",
        "# models (if you haven't already). Note that Lasso and ElasticNet are much\n",
        "# slower to fit, compared to Ridge.\n",
        "<YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8MqkJ4esNJ",
        "colab_type": "text"
      },
      "source": [
        "# outliers impact on regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4QiVwKesNK",
        "colab_type": "text"
      },
      "source": [
        "Remember, that when we minimise loss\n",
        "$$\n",
        "MSE = (\\hat y - y)^2\n",
        "$$\n",
        "\n",
        "which penalise more to for heigher values of error. What is that impact of this for the dataset with outliers, what do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4DRSJ_esNL",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of regression fitted with ordinary LR and RANSAC, which iteratively trains on random subsample of data, trying to identify outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwDJZxuBesNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn import linear_model, datasets\n",
        "\n",
        "\n",
        "n_samples = 1000\n",
        "n_outliers = 50\n",
        "\n",
        "\n",
        "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
        "                                      n_informative=1, noise=10,\n",
        "                                      coef=True, random_state=0)\n",
        "\n",
        "# Add outlier data\n",
        "np.random.seed(0)\n",
        "X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
        "y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
        "\n",
        "# Fit line using all data\n",
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Robustly fit linear model with RANSAC algorithm\n",
        "ransac = linear_model.RANSACRegressor()\n",
        "ransac.fit(X, y)\n",
        "inlier_mask = ransac.inlier_mask_\n",
        "outlier_mask = np.logical_not(inlier_mask)\n",
        "\n",
        "# Predict data of estimated models\n",
        "line_X = np.arange(X.min(), X.max())[:, np.newaxis]\n",
        "line_y = lr.predict(line_X)\n",
        "line_y_ransac = ransac.predict(line_X)\n",
        "\n",
        "# Compare estimated coefficients\n",
        "print(\"Estimated coefficients (true, linear regression, RANSAC):\")\n",
        "print(coef, lr.coef_, ransac.estimator_.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISdoPPi3esNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "lw = 2\n",
        "plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',\n",
        "            label='Inliers')\n",
        "plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',\n",
        "            label='Outliers')\n",
        "plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')\n",
        "plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,\n",
        "         label='RANSAC regressor')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Response\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJkJpSK4qNzl",
        "colab_type": "text"
      },
      "source": [
        "## Bonus part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZBAqmrrqQch",
        "colab_type": "text"
      },
      "source": [
        "Try using `sklearn.linear_model.SGDRegressor` with `huber` loss in the code above instead of `LinearRegression`. Is it better in this case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa1S9pwWesNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
